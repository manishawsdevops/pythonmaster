{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b6f4875",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-aad18742ed10>:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-07378a6047d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSparkFiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\Data_Engineering\\awsdataengineering\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\Desktop\\Data_Engineering\\awsdataengineering\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                     \u001b[1;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m    343\u001b[0m                         \u001b[1;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                         \u001b[1;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-aad18742ed10>:5 "
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext,SparkConf,SparkFiles\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9706bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logFile = \"file:///C:/Temp/README.md\"\n",
    "# logData = sc.textFile(logFile).cache()\n",
    "# numAs = logData.filter(lambda s: 'a' in s).count()\n",
    "# numBs = logData.filter(lambda s: 'b' in s).count()\n",
    "# print(numAs, numBs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e138407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "['scala', 'java', 'hadoop', 'spark', 'akka', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "# Creating an RDD - Resilient Distributed Dataset\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "counts = words.count()\n",
    "print(counts)\n",
    "coll = words.collect()\n",
    "print(coll)\n",
    "#for each we can do an operation like this\n",
    "words.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17e45842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "#Using filter in spark\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "words_filter = words.filter(lambda x: 'spark' in x)\n",
    "filtered = words_filter.collect()\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf91ae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('scala', 'Manish', 'Dodda'), ('java', 'Manish', 'Dodda'), ('hadoop', 'Manish', 'Dodda'), ('spark', 'Manish', 'Dodda'), ('akka', 'Manish', 'Dodda'), ('spark vs hadoop', 'Manish', 'Dodda'), ('pyspark', 'Manish', 'Dodda'), ('pyspark and spark', 'Manish', 'Dodda')]\n"
     ]
    }
   ],
   "source": [
    "#Map function can add a value and form a key value pair.\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "words_map = words.map(lambda x: (x,'Manish','Dodda'))\n",
    "mapped_values = words_map.collect()\n",
    "print(mapped_values)\n",
    "# The above is nothing but a list if first converted into a RDD data structure and then a value is added to every item in the \n",
    "# RDD dataaset and made a list of key value pairs inside the RDD dataset. In the same way we can add multiple items to evry item in the\n",
    "# RDD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ebe0404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# Reduce function in the RDD.\n",
    "# We can reduce a given RDD into a single or an agrregate using reduce function.\n",
    "# look at the below operation carefully.\n",
    "from operator import add\n",
    "nums = sc.parallelize([1,2,3,4,5])\n",
    "adding = nums.reduce(add)\n",
    "# If you observe above the nums is an Iterable which is RDD dataset, When we use the reduce method on it with a function\n",
    "# name while calling, num.reduce will iterate over every item in the RDD dataset and call add using that value.So, this \n",
    "# will result the addition of all the items in the RDD dataset.\n",
    "print(adding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4606662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dodda', (100, 60)), ('Gandhi', (2, 3)), ('Manish', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "# Join methods on the RDD dataset\n",
    "# Join will return a dataset with a pair of elements with matching keys and all the values in the matching keys.\n",
    "x = sc.parallelize([('Manish',1),('Gandhi',2),('Dodda',100)])\n",
    "y = sc.parallelize([('Manish',2),('Gandhi',3),('Dodda',60,30)])\n",
    "joined = x.join(y)\n",
    "print(joined.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aae0cb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Caching the practice of persist this RDD in Memory_cache. We can check if the RDD is cached or not like below.\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ") \n",
    "words.cache() \n",
    "caching = words.persist().is_cached \n",
    "print(caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e67fce51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scala', 'java', 'hadoop', 'spark', 'akka']\n",
      "hadoop\n"
     ]
    }
   ],
   "source": [
    "#Braodcast is the variable used to save a copy of RDD on the machine and not sent on machine with tasks.\n",
    "words_new = sc.broadcast([\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\"]) \n",
    "data = words_new.value \n",
    "print(data) \n",
    "elem = words_new.value[2] \n",
    "print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98a3e3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "#Accumulator\n",
    "num = sc.accumulator(10) \n",
    "def f(x): \n",
    "   global num \n",
    "   num+=x \n",
    "rdd = sc.parallelize([20,30,40,50]) \n",
    "rdd.foreach(f) \n",
    "final = num.value \n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86039425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmanishgandhi\\AppData\\Local\\Temp\\spark-4c840100-9e31-40e3-a58d-c1bb20201e95\\userFiles-6e182591-2cfe-4ae6-9cae-2d44f24f5da8\\README.md\n"
     ]
    }
   ],
   "source": [
    "# We are having two methods on SparkFiles which will give the filename and directory as get and getrootdirectory()\n",
    "finddistance = \"C:\\\\Temp\\\\README.md\"\n",
    "finddistancename = \"README.md\"\n",
    "sc.addFile(finddistance)\n",
    "print(SparkFiles.get(finddistancename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e35962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
